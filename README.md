# ğŸ¤– prompt-pwn-lab

[![License: MIT](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)
[![Python](https://img.shields.io/badge/python-3.10-blue.svg)](https://www.python.org/)
[![LLM Security Lab](https://img.shields.io/badge/type-LLM%20Security%20Lab-red)]()

> A red/blue team lab for testing prompt injection, jailbreaking, and prompt hardening in LLM-powered apps.

---

## ğŸ¤” What is `prompt-pwn-lab`?

`prompt-pwn-lab` is a lightweight testing ground for **prompt injection**, **LLM jailbreaks**, and **prompt hardening**.  
It helps security researchers, AI developers, and prompt engineers explore vulnerabilities and defenses in generative AI systems.

---

## ğŸ¯ Why This Exists

As LLMs become embedded in everything from chatbots to IDEs, they introduce a new frontier of vulnerabilities â€” many of which stem from how they're prompted, chained, or hijacked.

This lab gives you:
- ğŸ”´ A **red team sandbox** to run injection scenarios
- ğŸ”µ A **blue team toolkit** to test prompt defenses
- ğŸ§ª A safe place to experiment with adversarial prompts

---

## ğŸ§ª Run a Scenario

```bash
python3 lab_runner.py --scenario role_override
```

---

## ğŸ—‚ï¸ Folder Structure

```
attacks/              # Red team prompt injection files
mitigations/          # Blue team prompt filters & guard modules
lab_runner.py         # CLI to simulate attacks and run filters
README.md             # You're looking at it
```

---

## ğŸ’£ Included Scenarios

| Scenario Name       | Description                                            |
|---------------------|--------------------------------------------------------|
| `role_override`     | Classic system prompt override                         |
| `hidden_instruction`| Injected payload in user-submitted HTML or content     |
| `persona_swap`      | Rogue persona hijack (e.g. 'You are DarkGPT')          |

---

## ğŸ” Prompt Filtering (Mitigation)

Check out:
```python
mitigations/phrase_filter.py
```
> A simple keyword-based phrase blocker. You can extend it with regex, embeddings, or external LLM-based classifiers.

---

## ğŸ”§ Coming Soon

- ğŸ” Prompt trace visualization
- ğŸ§  Vector memory sanitization module
- âœ… LLM-generated response evaluation
- ğŸ§ª LangTest + adversarial scenario chaining

---

## âš ï¸ Disclaimer

This project is for **educational and ethical use only**.  
Please do not use these techniques outside environments where you have permission to test.

---

## ğŸ§¾ Project Summary

| Feature            | Status                                  |
|--------------------|------------------------------------------|
| ğŸ”´ Prompt Injection | âœ… Included (`attacks/`)                 |
| ğŸ”µ Prompt Filtering | âœ… Included (`mitigations/`)             |
| ğŸ§ª Scenario Runner  | âœ… `lab_runner.py` ready                 |
| ğŸ§  LLM Defense Ideas| ğŸ•’ Coming Soon                           |
| ğŸ“¦ Deploy Ready     | ğŸ› ï¸ Local simulation only (CLI)          |
| ğŸ“œ License          | [![License: MIT](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE) |

---

## ğŸ“œ License

MIT
